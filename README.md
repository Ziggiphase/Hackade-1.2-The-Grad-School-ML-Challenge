````markdown
# ðŸŽ“ From Scores to Seats: The Grad School ML Challenge
### ðŸ¥‰ Second Runner-Up (3rd Place) Solution

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Library](https://img.shields.io/badge/LightGBM-DART-orange)
![Tools](https://img.shields.io/badge/Tools-LazyPredict-purple)
![Status](https://img.shields.io/badge/Status-Completed-success)

This repository contains the source code for my 3rd place solution to the **Hackade - From Scores to Seats** challenge. The goal was to predict graduate school admission probability based on academic profiles.

## ðŸ§  The Approach
My solution focused on robust automated model selection followed by tuning a specific gradient boosting architecture.

### 1. Data Preprocessing
* **Imputation:** Handled missing values in `GRE Score`, `TOEFL Score`, `SOP`, and `CGPA` using mean imputation to preserve data distribution.
* **Categorical Encoding:** Applied One-Hot Encoding to the `Location` feature (using `get_dummies` with `drop_first=True`) to convert geographical data into machine-readable binary vectors.
* **Data Splitting:** Used an 80/20 Train-Validation split to ensure the model was not overfitting to the training data.

### 2. Model Selection (AutoML)
To identify the strongest candidate models quickly, I utilized **LazyPredict**.
* This allowed me to benchmark multiple algorithms (Random Forest, SVC, XGBoost, etc.) simultaneously on the validation set.
* *Outcome:* Tree-based boosting methods showed the highest potential.

### 3. The Winning Model: LightGBM (DART)
Based on the benchmarks, I implemented a **LightGBM Classifier** with a specific configuration:
* **Boosting Type:** `dart` (Dropouts meet Multiple Additive Regression Trees).
* **Why DART?** Unlike standard gradient boosting (gbdt), DART incorporates dropout (randomly dropping trees during training). This acts as a regularizer, significantly reducing overfitting and improving the model's ability to generalize to unseen test data.

## ðŸ“Š Evaluation & Interpretability

* **Metric:** Accuracy
* **Validation Score:** [INSERT YOUR VALIDATION ACCURACY HERE, e.g., 0.92]
* **Feature Importance:** The model identified key drivers for admission. As expected, `CGPA` and `GRE Score` were dominant factors, but the model also effectively utilized the encoded `Location` features.

## ðŸ› ï¸ Installation & Usage

1.  **Clone the repo:**
    ```bash
    git clone [https://github.com/Ziggiphase/Hackade-1.2-The-Grad-School-ML-Challenge.git](https://github.com/Ziggiphase/Hackade-1.2-The-Grad-School-ML-Challenge.git)
    cd Hackade-1.2-The-Grad-School-ML-Challenge
    ```

2.  **Install dependencies:**
    ```bash
    pip install pandas numpy matplotlib seaborn scikit-learn lightgbm lazypredict
    ```

3.  **Run the Notebook:**
    Open the Jupyter notebook to replicate the training process:
    ```bash
    jupyter notebook
    ```

## ðŸ“‚ File Structure
* `train.csv` / `test.csv`: The dataset.
* `notebook.ipynb`: The end-to-end pipeline (EDA -> LazyPredict -> LightGBM -> Submission).
* `lgbm2_submission.csv`: The final submission file generated by the model.

---
*Author: [Ziggiphase](https://github.com/Ziggiphase)*
````



